{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvztvbrhy1VTRYj3HonTEA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **예제 2.1 토큰화 코드**\n","\n","> 텍스트를 숫자 아이디로 바꾸는 과정\n","> 단어 단위(띄어쓰기 단위)로 토큰화 수행"],"metadata":{"id":"VlV3R-xfoTLf"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7GTD87VVwf9","executionInfo":{"status":"ok","timestamp":1745666869960,"user_tz":-540,"elapsed":33,"user":{"displayName":"황민지","userId":"13848132532084263606"}},"outputId":"baa02a1b-ec4a-4993-f668-47b85b38ee48"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_text_list :  ['나는', '최근', '파리', '여행을', '다녀왔다']\n","str2idx :  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n","idx2str :  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n","input_ids :  [0, 1, 2, 3, 4]\n"]}],"source":["# 띄어쓰기 단위로 분리\n","input_text = \"나는 최근 파리 여행을 다녀왔다\"\n","input_text_list = input_text.split()\n","print(\"input_text_list : \", input_text_list)\n","\n","# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n","str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n","idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n","print(\"str2idx : \", str2idx)\n","print(\"idx2str : \", idx2str)\n","\n","# 토큰을 토큰 아이디로 변환\n","input_ids = [str2idx[word] for word in input_text_list]\n","print(\"input_ids : \", input_ids)"]},{"cell_type":"markdown","source":["# **예제 2.2 토큰 아이디에서 벡터로 변환**\n","> 토큰의 의미를 담기 위해 벡터(숫자 집합)으로 변환"],"metadata":{"id":"2mwGrkUGo_0K"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","embedding_dim = 16\n","embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n","\n","input_embeddings = embed_layer(torch.tensor(input_ids)) # (5,16)\n","input_embeddings = input_embeddings.unsqueeze(0) # (1,5,16)\n","input_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xy2y3xrPbVDc","executionInfo":{"status":"ok","timestamp":1745666880986,"user_tz":-540,"elapsed":10988,"user":{"displayName":"황민지","userId":"13848132532084263606"}},"outputId":"727b4453-859a-46a1-9fe1-faca73282eed"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 16])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# **예제 2.3 절대적 위치 인코딩**\n","> 새로운 임베딩 층을 추가 -> 위치 인덱스에 따라 임베딩을 더함"],"metadata":{"id":"phuKH2MtpOxS"}},{"cell_type":"code","source":["embedding_dim = 16\n","max_position = 12\n","embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n","position_embed_layer = nn.Embedding(max_position, embedding_dim)\n","\n","position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n","position_encodings = position_embed_layer(position_ids)\n","token_embeddings = embed_layer(torch.tensor(input_ids)) # (5,16)\n","token_embeddings = token_embeddings.unsqueeze(0) # (1,5,16)\n","token_embeddings = token_embeddings + position_encodings\n","input_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTHfMIpPjzun","executionInfo":{"status":"ok","timestamp":1745666881015,"user_tz":-540,"elapsed":23,"user":{"displayName":"황민지","userId":"13848132532084263606"}},"outputId":"dfcda443-17de-4aaf-b4c6-c04faeb0fd8e"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 16])"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# **예제 2.4 쿼리, 키, 값 벡터 만드는 nn.Linear 층**\n","> 가중치 -> nn.Linear 층 사용       \n","> 각 가중치 생성 -> weight_q, weight_k, weight_v     \n","> 입력 → input_embedding\n","\n","⇒ 선형 층에 통과시켜 쿼리, 키, 값 생성"],"metadata":{"id":"dI5Floq6pavm"}},{"cell_type":"code","source":["head_dim = 16\n","\n","# 쿼리, 키, 값을 계산하기 위하 변환\n","weight_q = nn.Linear(embedding_dim, head_dim)\n","weight_k = nn.Linear(embedding_dim, head_dim)\n","weight_v = nn.Linear(embedding_dim, head_dim)\n","\n","# 변환 수행\n","querys = weight_q(input_embeddings) # (1,5,16)\n","keys = weight_k(input_embeddings) # (1,5,16)\n","values = weight_v(input_embeddings) # (1,5,16)"],"metadata":{"id":"NNKw2V3TnnW6","executionInfo":{"status":"ok","timestamp":1745666881062,"user_tz":-540,"elapsed":46,"user":{"displayName":"황민지","userId":"13848132532084263606"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **예제 2.5 스케일 점곱 방식의 어텐션**\n","> 단어와의 관계를 얼마나 반영할지 명확하게 정하기 어렵기에 합을 1로 만들 수 있도록 소프트 맥스 취하기"],"metadata":{"id":"yiWC9O0gqO4B"}},{"cell_type":"code","source":["from math import sqrt\n","import torch.nn.functional as F\n","\n","def compute_attention(querys, keys, values, is_causal=False):\n","\tdim_k = querys.size(-1) # 16\n","\tscores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n","\tweights = F.softmax(scores, dim=-1)\n","\treturn weights @ values"],"metadata":{"id":"ALI60ZUQsH5C","executionInfo":{"status":"ok","timestamp":1745666881133,"user_tz":-540,"elapsed":60,"user":{"displayName":"황민지","userId":"13848132532084263606"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# **예제 2.6 어텐션 연산의 입력과 출력**\n","> 어텐션을 거치고 나면 입력과 형태는 동일하면서 주변 토큰과의 관련도에 따라 값 벡터를 조합한 새로운 토큰 임베딩 생성"],"metadata":{"id":"eLjrFBIgs0Cb"}},{"cell_type":"code","source":["print(\"원본 입력 형태: \", input_embeddings.shape)\n","\n","after_attention_embeddings = compute_attention(querys, keys, values)\n","\n","print(\"어텐션 적용 후 형태: \", after_attention_embeddings.shape)\n","\n","# 원본 입력 형태:  torch.Size([1, 5, 16])\n","# 어텐션 적용 후 형태:  torch.Size([1, 5, 16])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wAT5y7PBszyg","executionInfo":{"status":"ok","timestamp":1745666881134,"user_tz":-540,"elapsed":7,"user":{"displayName":"황민지","userId":"13848132532084263606"}},"outputId":"0e36533b-a29b-484a-cba0-1d00a3f02686"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["원본 입력 형태:  torch.Size([1, 5, 16])\n","어텐션 적용 후 형태:  torch.Size([1, 5, 16])\n"]}]},{"cell_type":"markdown","source":["# **예제 2.7 어텐션 연산을 수행하는 AttentionHead 클래스**\n","> __init__ 메서드 -> 선형 층(weight_q, weight_k, weight_v) 생성   \n","> forward 메서드 -> 선형 층을 통해 쿼리, 키, 값 벡터 생성    \n","> compute_attention 함수 사용 -> 어텐션 연산 수행"],"metadata":{"id":"MhemjVmCvj8G"}},{"cell_type":"code","source":["class AttentionHead(nn.Module):\n","  def __init__(self, token_embed_dim, head_dim, is_causal=False):\n","    super().__init__()\n","    self.is_causal = is_causal\n","    self.weight_q = nn.Linear(token_embed_dim, head_dim) # 쿼리 벡터 생성을 위한 선형 층\n","    self.weight_k = nn.Linear(token_embed_dim, head_dim) # 키 벡터 생성을 위한 선형 층\n","    self.weight_v = nn.Linear(token_embed_dim, head_dim) # 값 벡터 생성을 위한 선형 층\n","\n","  def forward(self, querys, keys, values):\n","    outputs = compute_attention(\n","        self.weight_q(querys),  # 쿼리 벡터\n","        self.weight_k(keys),    # 키 벡터\n","        self.weight_v(values),  # 값 벡터\n","        is_causal=self.is_causal\n","    )\n","    return outputs\n","\n","attention_head = AttentionHead(embedding_dim, embedding_dim)\n","after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)"],"metadata":{"id":"MdKhli3RsHwh","executionInfo":{"status":"ok","timestamp":1745666881191,"user_tz":-540,"elapsed":60,"user":{"displayName":"황민지","userId":"13848132532084263606"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# **예제 2.8 멀티 헤드 어텐션 구현**\n","> AttentionHead와 대부분의 코드가 동일하나\n","\n","1. 헤드 수만큼 연산을 수행하기 위해 쿼리, 키, 값을 n_head개로 쪼개기 -> 여러 선형 층\n","1. 각각의 어텐션 계산 -> h번의 스케일 점곱 어텐션\n","2. 입력과 같은 형태로 다시 변환 -> 어텐션 결과 연결\n","3. 마지막으로 선형층 통과시키고 최종 결과 반환 -> 마지막 선형 층\n"],"metadata":{"id":"8zGVmD7JxIzS"}},{"cell_type":"code","source":["class MultiheadAttention(nn.Module):\n","  def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n","    super().__init__()\n","    self.n_head = n_head\n","    self.is_causal = is_causal\n","    self.weight_q = nn.Linear(token_embed_dim, d_model)\n","    self.weight_k = nn.Linear(token_embed_dim, d_model)\n","    self.weight_v = nn.Linear(token_embed_dim, d_model)\n","    self.concat_linear = nn.Linear(d_model, d_model)\n","\n","  def forward(self, querys, keys, values):\n","    B, T, C = querys.size()\n","    querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","    keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","    values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","    attention = compute_attention(querys, keys, values, self.is_causal)\n","    output = attention.transpose(1, 2).contiguous().view(B, T, C)\n","    output = self.concat_linear(output)\n","    return output\n","\n","n_head = 4\n","mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n","after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n","after_attention_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYXbBYSMxJTB","executionInfo":{"status":"ok","timestamp":1745666881200,"user_tz":-540,"elapsed":66,"user":{"displayName":"황민지","userId":"13848132532084263606"}},"outputId":"0148b2b5-9d19-44e3-bd82-5c4e9818a7e7"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 16])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# **예제 2.9 층 정규화 코드**\n","> 파이토치가 제공하는 LayerNorm 클래스 이용     \n","> -> nn.LayerNorm 클래스로 층 정규화 레이스 만들기    \n","> (토큰 임베딩 차원(embedding_dim)인자로 전달, 입력 임베딩을 층 정규화 레이어에 통과시켜 정규화된 임베딩(norm_x)로 만들기)"],"metadata":{"id":"VVo0xJ7RcHiC"}},{"cell_type":"code","source":["norm = nn.LayerNorm(embedding_dim)\n","norm_x = norm(input_embeddings)\n","norm_x.shape # torch.Size([1,5,16])\n","\n","norm_x.mean(dim=-1).data, norm_x.std(dim=-1).data # 실제로 평균과 표준편차 확인하기\n","\n","# (tensor([[7.4506e-09, 0.0000e+00, 1.1176e-08, 2.9802e-08, 1.1176e-08]]),\n","# tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZTVCGzAcLXK","executionInfo":{"status":"ok","timestamp":1745666881231,"user_tz":-540,"elapsed":30,"user":{"displayName":"황민지","userId":"13848132532084263606"}},"outputId":"89d79fbf-962c-4d2b-905a-6008ab275865"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-1.8626e-08,  0.0000e+00,  2.2352e-08, -2.2352e-08, -1.6764e-08]]),\n"," tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# **예제 2.10 피드 포워드 층 코드**\n",">**구성**\n","- 선형 층\n","- 드롭아웃 층\n","- 층 정규화\n","- 활성 함수"],"metadata":{"id":"bTz-8rtQcK41"}},{"cell_type":"code","source":["class PreLayerNormFeedForward(nn.Module) :\n","\tdef __init_(self, d_model, dim_feedforward, dropout):\n","\t\tsuper().__init_()\n","\t\tself.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1\n","\t\tself.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2\n","\t\tself.dropout1 = nn.Dropout(dropout) # 드롭아웃 층 1\n","\t\tself.dropout2 = nn.Dropout(dropout) # 드롭아웃 층 2\n","\t\tself.activation = nn.GELU() # 활성 함수\n","\t\tself.norm = nn.LayerNorm(d_model) # 층 정규화\n","\n","def forward(self, src):\n","  x = self.norm(src)\n","  x = x + self. linear2(self.dropoutl(self.activation(self.linear1(x))))\n","  x = self.dropout2(x)\n","  return x"],"metadata":{"id":"y-syeBvuef51","executionInfo":{"status":"ok","timestamp":1745667142961,"user_tz":-540,"elapsed":14,"user":{"displayName":"황민지","userId":"13848132532084263606"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# **예제 2.11 인코더 층**\n","- 입력인 src를 self.norm1을 통해 층 정규화\n","- 멀티 헤드 어텐션 클래스를 인스턴스화한 self.attn을 통해 멀티 헤드 어텐션 연산 수행\n","- 잔차 연결을 위해 어텐션 결과에 드롭아웃을 취한 self.dropout1(attn_output)과 입력(src)을 더해준다\n","- self.feed_forward(x)를 통해 피드 포워드 연산"],"metadata":{"id":"O6SMNOUc1hIA"}},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","  def __init__(self, d_model, nhead, dim_feedforward, dropout):\n","    super().__init__()\n","    self.self_attn = MultiheadAttention(d_model, d_model, nhead)\n","    self.linear1 = nn.Linear(d_model)\n","    self.dropout = nn.Dropout(dropout)\n","    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n","\n","def forward(self, src):\n","  norm_x = self.norm1(src)\n","  attrn_output = self.attn(norm_x, norm_x, norm_x)\n","  x = src + self.dropout1(attn_output)\n","\n","  # 피드 포워드\n","  x = self.feed_forward(x)\n","  return x"],"metadata":{"id":"K_2ovqy9hE14","executionInfo":{"status":"ok","timestamp":1745666885351,"user_tz":-540,"elapsed":4,"user":{"displayName":"황민지","userId":"13848132532084263606"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# **예제 2.12 인코더 구현**\n"],"metadata":{"id":"lEJQaW672CS-"}},{"cell_type":"code","source":["import copy\n","def get_clones(module, N):\n","  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","class TransformerEncoder(nn.Module):\n","  def __init__(self, encoder_layer, num_layers):\n","    super().__init__()\n","    self.layers = get_clones(encoder_layer, num_layers)\n","    self.num_layers = num_layers\n","    self.norm = norm\n","\n","  def forward(self, src):\n","    output = src\n","    for mod in self.layers:\n","      output = mod(output)\n","    return output"],"metadata":{"id":"xxwFqW1V2HqE","executionInfo":{"status":"ok","timestamp":1745666887177,"user_tz":-540,"elapsed":7,"user":{"displayName":"황민지","userId":"13848132532084263606"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# **예제 2.13 디코더에서 어텐션 연산(마스크 어텐션)**\n"],"metadata":{"id":"0HnCThcF3yb4"}},{"cell_type":"code","source":["def compute_attention(querys, keys, values, is_causal=False):\n","  dim_k = querys.size(-1) # 16\n","  scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k) # (1,5,5)\n","  if is_causal:\n","    query_length = querys.size(-2)\n","    key_length = keys.size(-2)\n","    temp_mask = torch.ones(query_length, key_length,\n","                           dtype=torch.bool).trill(diagonal=0)\n","    scores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n","  weights = F.softmax(scores, dim=-1) # (1,5,5)\n","  return weigths @ values # (1,5,16)"],"metadata":{"id":"PKoT87Am382Z","executionInfo":{"status":"ok","timestamp":1745667449896,"user_tz":-540,"elapsed":8,"user":{"displayName":"황민지","userId":"13848132532084263606"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# **예제 2.14 크로스 어텐션이 포함된 디코더 층**"],"metadata":{"id":"WOKMxiCe45pU"}}]}